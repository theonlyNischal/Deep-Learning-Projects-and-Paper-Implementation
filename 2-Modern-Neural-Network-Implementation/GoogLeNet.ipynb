{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to paper: [Going deeper with convolutions/Inception/GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "\n",
    "Andrew Ng's Video: [Inception Network Motivaion](https://www.youtube.com/watch?v=C86ZXvgpejM)\n",
    "\n",
    "\n",
    "GoogLeNet is one of the incarnation of Inception architecture and it has won ILSVRC14 while VGGNet was slight behind. Most of the authors were from Google and this might be the reason that emphasis was given for computational cost while not compromising with the quality.\n",
    "\n",
    "Inception Architecture answers two questions:\n",
    "\n",
    "### 1. Which size convolutional kernels are best?\n",
    "\n",
    "There has been a lot of experiment around the kernel size. Many popular neural network has used kernels ranging from 11x11 to 1x1. Kernels of different size will learn different features from input image or input activation maps. What Inception Network proposes is instead of using certain kernel what if we use combination of such kernels and let them learn what they want. Using such combination increases our chance of finding best weights which will be used to do tasks like classificatin, recognition etc.\n",
    "\n",
    "![Combination of Filters](assets/combination_of_filters.png)\n",
    "\n",
    "The idea here is to use combination of 1x1, 3x3, 5x5 filter. At that time, most popular and sucessfull networks have used max-pooling layer and it was believed that max-pooling is compulsary to do well on ILSVRC types problems. But [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806) have shown that **max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy.** Since the original architecture have used max-pooling layer we will use the same convention.\n",
    "\n",
    "The output activation map is the combination of 1x1, 3x3, 5x5 and a max-pool filters. **Same** padding has been used to make sure that height and width after applying all filters remain same. If we apply 64 1x1 filters, we would get output activation map of size 28x28x64(shown as green), if we apply 128 3x3 filters we would get output activation map of size 28x28x128. By doing so, we would get output activation maps of size 28x28x256 and this activation map will be passed to next layer.\n",
    "\n",
    "**The answer to what size convolutional kernels are best : We will use combination of kernels and let the network learn what is best based on optimization algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How will you make your network less resource hungry?\n",
    "One big problem with using such combination of kernels is that it will be very resource hungry. In above figure, if we look at a 5x5 convolution, it gives output of size 28x28x32. So 5x5x192 kernels are used to give 28x28x32 activation maps. Total computation requred is 28x28x32x5x5x192 = 120422400(>120 millions). So without dimension reduction our network will blow up within next stages.\n",
    "\n",
    "So, we will use 1x1 convolution as a mean to reduce the computational cost before heavy 3x3 and 5x5 convolution.\n",
    "\n",
    "![inception_naive_vs_dimension_reduction](assets/inception_naive_vs_dimension_reduction.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such inception blocks are repeated a number of times to define a network architecture. We can see the repetition of inception blocks in the GoogleNet:\n",
    "\n",
    "### GoogLeNet\n",
    "GoogLeNet is most successfull incarnation of Inception Architecture.\n",
    "\n",
    "![GoogLeNet](assets/googlenet.png)\n",
    "\n",
    "#### Why auxiliary classifiers at the intermediate layers?\n",
    "These auxiliary classifiers takes some hidden layers and tries to make a prediction. The gradients are added and gets propagated back. This makes sure that the feature learned at the hidden layers are not completely useless\n",
    "\n",
    "\n",
    "\n",
    "**Compact GoogLeNet**\n",
    "\n",
    "![Compact GoogLeNet](assets/compact_googlenet.png)\n",
    "\n",
    "**GoogLeNet System Architecture**\n",
    "![GoogLeNet_system_architecture](assets/GoogLeNet_system_architecture.png)\n",
    "\n",
    "### GoogLeNet in PyTorch\n",
    "\n",
    "As we see, the basic convolutional block in GoogLeNet is Inception block which allows us to use combination of kernels. We will start by designing Inception block and use it define whole GoogLeNet architecture.\n",
    "\n",
    "![Inception block](assets/inception_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 4 parallel path and output of each path is concatenated at the end."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.in_channels = config[0]\n",
    "        self.c1_out = config[1]\n",
    "        self.c2_out = config[2]\n",
    "        self.c3_out = config[3]\n",
    "        self.c4_out = config[4]\n",
    "        \n",
    "        self.path1 = nn.Conv2d(self.in_channels, self.c1_out, kernel_size=1)\n",
    "        \n",
    "        self.path2_1 = nn.Conv2d(self.in_channels, self.c2_out[0], kernel_size=1)\n",
    "        self.path2_2 = nn.Conv2d(self.c2_out[0], self.c2_out[1], kernel_size=3, padding=1)\n",
    "        \n",
    "        self.path3_1 = nn.Conv2d(self.in_channels, self.c3_out[0], kernel_size=1)\n",
    "        self.path3_2 = nn.Conv2d(self.c3_out[0], self.c3_out[1], kernel_size=5, padding=2)\n",
    "        \n",
    "        self.path4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.path4_2 = nn.Conv2d(self.in_channels, self.c4_out, kernel_size=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        path1 = self.relu(self.path1(x))\n",
    "        path2 = self.relu(self.path2_2(self.relu(self.path2_1(x))))\n",
    "        path3 = self.relu(self.path3_2(self.relu(self.path3_1(x))))\n",
    "        path4 = self.relu(self.path4_2(self.path4_1(x)))\n",
    "        \n",
    "        return torch.cat((path1, path2, path3, path4), axis=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 10\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64,\n",
    "                              kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=64,\n",
    "                              kernel_size=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=192,\n",
    "                            kernel_size=3, padding=1)\n",
    "        \n",
    "        self.inception3 = self.get_inception_layers(\"type_3\", 2)\n",
    "        self.inception4 = self.get_inception_layers(\"type_4\", 5)\n",
    "        self.inception5 = self.get_inception_layers(\"type_5\", 2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "        self.linear = nn.Linear(1024, output_size)\n",
    "        \n",
    "    \n",
    "    def get_inception_layers(self, block_type, num_inception_blocks):\n",
    "        \n",
    "        layers = list()\n",
    "        \n",
    "        for i in range(num_inception_blocks):\n",
    "            layers.append(InceptionBlock(GoogLeNet_config[block_type][i]))\n",
    "            \n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.relu(self.conv2_2(self.relu(self.conv2_1(x))))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogLeNet_config = {\n",
    "    \"type_3\": [\n",
    "        [192, 64, (96, 128), (16, 32), 32],\n",
    "        [256, 128, (128, 192), (32, 96), 64]\n",
    "    ],\n",
    "    \n",
    "    \"type_4\":[\n",
    "        [480, 192, (96, 208), (16, 48), 64 ],\n",
    "        [512, 160, (112, 224), (24, 64), 64],\n",
    "        [512, 128, (128, 256), (24, 64), 64],\n",
    "        [512, 112, (144, 288), (32, 64), 64],\n",
    "        [528, 256, (160, 320), (32, 128), 128]\n",
    "    ],\n",
    "    \n",
    "    \"type_5\":[\n",
    "        [832, 256, (160, 320), (32, 128), 128],\n",
    "        [832, 384, (192, 384), (48, 128), 128]\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogLeNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (conv2_1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2_2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (inception3): Sequential(\n",
       "    (0): InceptionBlock(\n",
       "      (path1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): InceptionBlock(\n",
       "      (path1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inception4): Sequential(\n",
       "    (0): InceptionBlock(\n",
       "      (path1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): InceptionBlock(\n",
       "      (path1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): InceptionBlock(\n",
       "      (path1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): InceptionBlock(\n",
       "      (path1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): InceptionBlock(\n",
       "      (path1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inception5): Sequential(\n",
       "    (0): InceptionBlock(\n",
       "      (path1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): InceptionBlock(\n",
       "      (path1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_1): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path2_2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (path3_1): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (path3_2): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (path4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (path4_2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_googlenet = GoogLeNet(10)\n",
    "my_googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "my_googlenet = my_googlenet.double()\n",
    "\n",
    "X = np.random.uniform(size=(1, 3, 224, 224))\n",
    "X = torch.from_numpy(X)\n",
    "output = my_googlenet(X)\n",
    "\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Which is the required output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
